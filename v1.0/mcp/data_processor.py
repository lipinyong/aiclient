"""
æ•°æ®å¤„ç† MCP æœåŠ¡ - è§£å†³å¤§æ•°æ®ä¸Šä¸‹æ–‡è¶…é™é—®é¢˜
ä½¿ç”¨ Map-Reduce æ¨¡å¼åˆ†å—å¤„ç†å¤§é‡æ•°æ®
"""

import os
import json
import hashlib
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

logger = logging.getLogger(__name__)

CHARS_PER_TOKEN = 2.5
MAX_TOKENS_PER_CHUNK = 25000  # å‡å°å—å¤§å°ï¼Œä¸ºæ¶ˆæ¯å†å²ç•™å‡ºç©ºé—´
MAX_CHARS_PER_CHUNK = int(MAX_TOKENS_PER_CHUNK * CHARS_PER_TOKEN)  # çº¦ 62500 å­—ç¬¦

_cache_dir = None
_chunks = {}
_summaries = {}
_current_task = None  # å½“å‰å¤„ç†ä»»åŠ¡çŠ¶æ€


def _get_cache_dir() -> Path:
    """è·å–ç¼“å­˜ç›®å½•"""
    global _cache_dir
    if _cache_dir is None:
        _cache_dir = Path.home() / ".ai_chat_cli" / "cache"
        _cache_dir.mkdir(parents=True, exist_ok=True)
    return _cache_dir


def _estimate_tokens(text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    return int(len(text) / CHARS_PER_TOKEN)


def _generate_chunk_id(source: str, index: int) -> str:
    """ç”Ÿæˆæ•°æ®å—ID"""
    hash_input = f"{source}_{index}_{datetime.now().isoformat()}".encode()
    return hashlib.md5(hash_input).hexdigest()[:12]


def _save_chunk_content(chunk_id: str, content: str):
    """ä¿å­˜æ•°æ®å—å†…å®¹åˆ°ç¼“å­˜"""
    chunk_file = _get_cache_dir() / f"chunk_{chunk_id}.txt"
    with open(chunk_file, 'w', encoding='utf-8') as f:
        f.write(content)


async def chunk_text(text: str, source: str = "input") -> Dict[str, Any]:
    """å°†å¤§æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªæ•°æ®å—
    
    æ”¹è¿›çš„åˆ†å—ç­–ç•¥ï¼š
    1. ä¼˜å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²
    2. å¦‚æœå•è¡Œè¿‡é•¿ï¼ŒæŒ‰å›ºå®šå­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²
    """
    global _chunks
    
    chunks = []
    chunk_index = 0
    
    # å¦‚æœæ–‡æœ¬æ²¡æœ‰æ¢è¡Œç¬¦æˆ–æ¢è¡Œç¬¦å¾ˆå°‘ï¼ŒæŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²
    if text.count('\n') < len(text) / MAX_CHARS_PER_CHUNK:
        # æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²
        for i in range(0, len(text), MAX_CHARS_PER_CHUNK):
            chunk_text_content = text[i:i + MAX_CHARS_PER_CHUNK]
            chunk_id = _generate_chunk_id(source, chunk_index)
            
            chunk_info = {
                "chunk_id": chunk_id,
                "source": source,
                "start_pos": i,
                "end_pos": min(i + MAX_CHARS_PER_CHUNK, len(text)),
                "char_count": len(chunk_text_content),
                "estimated_tokens": _estimate_tokens(chunk_text_content),
                "preview": chunk_text_content[:200] + "..." if len(chunk_text_content) > 200 else chunk_text_content
            }
            chunks.append(chunk_info)
            _chunks[chunk_id] = chunk_info
            _save_chunk_content(chunk_id, chunk_text_content)
            chunk_index += 1
    else:
        # æŒ‰è¡Œåˆ†å‰²
        lines = text.split('\n')
        current_chunk = []
        current_chars = 0
        start_line = 0
        
        for i, line in enumerate(lines):
            line_chars = len(line) + 1
            
            # å¦‚æœå•è¡Œè¿‡é•¿ï¼Œéœ€è¦æ‹†åˆ†è¿™ä¸€è¡Œ
            if line_chars > MAX_CHARS_PER_CHUNK:
                # å…ˆä¿å­˜ä¹‹å‰çš„å†…å®¹
                if current_chunk:
                    chunk_text_content = '\n'.join(current_chunk)
                    chunk_id = _generate_chunk_id(source, chunk_index)
                    chunk_info = {
                        "chunk_id": chunk_id,
                        "source": source,
                        "start_line": start_line,
                        "end_line": i - 1,
                        "char_count": len(chunk_text_content),
                        "estimated_tokens": _estimate_tokens(chunk_text_content),
                        "preview": chunk_text_content[:200] + "..."
                    }
                    chunks.append(chunk_info)
                    _chunks[chunk_id] = chunk_info
                    _save_chunk_content(chunk_id, chunk_text_content)
                    chunk_index += 1
                    current_chunk = []
                    current_chars = 0
                
                # æ‹†åˆ†é•¿è¡Œ
                for j in range(0, len(line), MAX_CHARS_PER_CHUNK):
                    sub_line = line[j:j + MAX_CHARS_PER_CHUNK]
                    chunk_id = _generate_chunk_id(source, chunk_index)
                    chunk_info = {
                        "chunk_id": chunk_id,
                        "source": source,
                        "line": i,
                        "sub_part": j // MAX_CHARS_PER_CHUNK,
                        "char_count": len(sub_line),
                        "estimated_tokens": _estimate_tokens(sub_line),
                        "preview": sub_line[:200] + "..." if len(sub_line) > 200 else sub_line
                    }
                    chunks.append(chunk_info)
                    _chunks[chunk_id] = chunk_info
                    _save_chunk_content(chunk_id, sub_line)
                    chunk_index += 1
                
                start_line = i + 1
                continue
            
            if current_chars + line_chars > MAX_CHARS_PER_CHUNK and current_chunk:
                chunk_text_content = '\n'.join(current_chunk)
                chunk_id = _generate_chunk_id(source, chunk_index)
                
                chunk_info = {
                    "chunk_id": chunk_id,
                    "source": source,
                    "start_line": start_line,
                    "end_line": i - 1,
                    "char_count": len(chunk_text_content),
                    "estimated_tokens": _estimate_tokens(chunk_text_content),
                    "preview": chunk_text_content[:200] + "..." if len(chunk_text_content) > 200 else chunk_text_content
                }
                chunks.append(chunk_info)
                _chunks[chunk_id] = chunk_info
                _save_chunk_content(chunk_id, chunk_text_content)
                
                current_chunk = [line]
                current_chars = line_chars
                start_line = i
                chunk_index += 1
            else:
                current_chunk.append(line)
                current_chars += line_chars
        
        if current_chunk:
            chunk_text_content = '\n'.join(current_chunk)
            chunk_id = _generate_chunk_id(source, chunk_index)
            
            chunk_info = {
                "chunk_id": chunk_id,
                "source": source,
                "start_line": start_line,
                "end_line": len(lines) - 1,
                "char_count": len(chunk_text_content),
                "estimated_tokens": _estimate_tokens(chunk_text_content),
                "preview": chunk_text_content[:200] + "..." if len(chunk_text_content) > 200 else chunk_text_content
            }
            chunks.append(chunk_info)
            _chunks[chunk_id] = chunk_info
            _save_chunk_content(chunk_id, chunk_text_content)
    
    total_tokens = sum(c["estimated_tokens"] for c in chunks)
    
    return {
        "success": True,
        "total_chunks": len(chunks),
        "total_estimated_tokens": total_tokens,
        "chunks": chunks,
        "message": f"å·²å°†æ–‡æœ¬åˆ†å‰²æˆ {len(chunks)} ä¸ªæ•°æ®å—ï¼Œæ€»è®¡çº¦ {total_tokens} tokens"
    }


async def chunk_file(file_path: str) -> Dict[str, Any]:
    """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªæ•°æ®å—"""
    try:
        path = Path(file_path)
        if not path.is_absolute():
            docker_working_dir = Path("/app")
            if docker_working_dir.exists():
                data_dir = docker_working_dir / "data"
            else:
                data_dir = Path("data")
            path = data_dir / path
        
        if not path.exists():
            return {"success": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        return await chunk_text(content, source=str(path))
    except Exception as e:
        logger.error(f"åˆ†å—æ–‡ä»¶å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


async def chunk_directory(dir_path: str, pattern: str = "*.txt") -> Dict[str, Any]:
    """å°†ç›®å½•ä¸­çš„å¤šä¸ªæ–‡ä»¶åˆ†å‰²æˆæ•°æ®å—"""
    try:
        path = Path(dir_path)
        if not path.is_absolute():
            docker_working_dir = Path("/app")
            if docker_working_dir.exists():
                data_dir = docker_working_dir / "data"
            else:
                data_dir = Path("data")
            path = data_dir / path
        
        if not path.exists():
            return {"success": False, "error": f"ç›®å½•ä¸å­˜åœ¨: {dir_path}"}
        
        all_chunks = []
        total_tokens = 0
        files_processed = 0
        
        for file_path in sorted(path.glob(pattern)):
            if file_path.is_file():
                result = await chunk_file(str(file_path))
                if result.get("success"):
                    all_chunks.extend(result.get("chunks", []))
                    total_tokens += result.get("total_estimated_tokens", 0)
                    files_processed += 1
        
        return {
            "success": True,
            "files_processed": files_processed,
            "total_chunks": len(all_chunks),
            "total_estimated_tokens": total_tokens,
            "chunks": all_chunks,
            "message": f"å·²å¤„ç† {files_processed} ä¸ªæ–‡ä»¶ï¼Œåˆ†å‰²æˆ {len(all_chunks)} ä¸ªæ•°æ®å—"
        }
    except Exception as e:
        logger.error(f"åˆ†å—ç›®å½•å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


async def get_chunk(chunk_id: str) -> Dict[str, Any]:
    """è·å–æŒ‡å®šæ•°æ®å—çš„å®Œæ•´å†…å®¹"""
    try:
        chunk_file = _get_cache_dir() / f"chunk_{chunk_id}.txt"
        if chunk_file.exists():
            with open(chunk_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunk_info = _chunks.get(chunk_id, {})
            return {
                "success": True,
                "chunk_id": chunk_id,
                "content": content,
                "char_count": len(content),
                "estimated_tokens": _estimate_tokens(content),
                "info": chunk_info
            }
        return {"success": False, "error": f"æ•°æ®å—ä¸å­˜åœ¨: {chunk_id}"}
    except Exception as e:
        logger.error(f"è·å–æ•°æ®å—å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


async def save_summary(chunk_id: str, summary: str, key_points: List[str] = None) -> Dict[str, Any]:
    """ä¿å­˜æ•°æ®å—çš„æ‘˜è¦ç»“æœï¼Œå¹¶è‡ªåŠ¨æ ‡è®°ä¸ºå·²å¤„ç†"""
    global _summaries
    
    try:
        summary_data = {
            "chunk_id": chunk_id,
            "summary": summary,
            "key_points": key_points or [],
            "created_at": datetime.now().isoformat()
        }
        _summaries[chunk_id] = summary_data
        
        summary_file = _get_cache_dir() / f"summary_{chunk_id}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # è‡ªåŠ¨æ ‡è®°ä¸ºå·²å¤„ç†
        mark_result = await mark_chunk_processed(chunk_id)
        
        # è·å–å¤„ç†è¿›åº¦ä¿¡æ¯
        task = _load_current_task()
        progress_info = ""
        if task:
            chunk_ids = task.get("chunk_ids", [])
            processed_ids = task.get("processed_ids", [])
            remaining = len(chunk_ids) - len(processed_ids)
            if remaining > 0:
                progress_info = f"ã€‚è¿˜å‰© {remaining} ä¸ªæ•°æ®å—å¾…å¤„ç†ï¼Œè¯·ç»§ç»­è°ƒç”¨ dataproc_get_next_chunk è·å–ä¸‹ä¸€ä¸ª"
            else:
                progress_info = "ã€‚æ‰€æœ‰æ•°æ®å—å·²å¤„ç†å®Œæˆï¼è¯·è°ƒç”¨ dataproc_merge_summaries ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"
        
        return {
            "success": True,
            "message": f"å·²ä¿å­˜æ•°æ®å— {chunk_id} çš„æ‘˜è¦{progress_info}",
            "progress": mark_result if mark_result.get("success") else None
        }
    except Exception as e:
        logger.error(f"ä¿å­˜æ‘˜è¦å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


async def get_all_summaries() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰å·²ä¿å­˜çš„æ•°æ®å—æ‘˜è¦"""
    try:
        summaries = []
        cache_dir = _get_cache_dir()
        
        for summary_file in cache_dir.glob("summary_*.json"):
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary_data = json.load(f)
                summaries.append(summary_data)
        
        combined_text = "\n\n".join([
            f"ã€{s.get('chunk_id', 'unknown')}ã€‘\n{s.get('summary', '')}"
            for s in summaries
        ])
        
        return {
            "success": True,
            "total_summaries": len(summaries),
            "summaries": summaries,
            "combined_text": combined_text,
            "combined_tokens": _estimate_tokens(combined_text)
        }
    except Exception as e:
        logger.error(f"è·å–æ‘˜è¦å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


async def estimate_tokens(text: str) -> Dict[str, Any]:
    """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    tokens = _estimate_tokens(text)
    return {
        "success": True,
        "char_count": len(text),
        "estimated_tokens": tokens,
        "exceeds_limit": tokens > 131072,
        "recommended_chunks": max(1, tokens // MAX_TOKENS_PER_CHUNK + 1)
    }


async def clear_cache() -> Dict[str, Any]:
    """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„æ•°æ®å—å’Œæ‘˜è¦"""
    global _chunks, _summaries, _current_task
    
    try:
        cache_dir = _get_cache_dir()
        count = 0
        
        for f in cache_dir.glob("chunk_*.txt"):
            f.unlink()
            count += 1
        for f in cache_dir.glob("summary_*.json"):
            f.unlink()
            count += 1
        for f in cache_dir.glob("task_*.json"):
            f.unlink()
            count += 1
        
        _chunks.clear()
        _summaries.clear()
        _current_task = None
        
        return {
            "success": True,
            "message": f"å·²æ¸…ç† {count} ä¸ªç¼“å­˜æ–‡ä»¶"
        }
    except Exception as e:
        logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def _save_task_state(task_id: str, chunk_ids: List[str], processed_ids: List[str], 
                     description: str = "", source: str = ""):
    """ä¿å­˜ä»»åŠ¡çŠ¶æ€åˆ°æ–‡ä»¶"""
    global _current_task
    task_data = {
        "task_id": task_id,
        "chunk_ids": chunk_ids,
        "processed_ids": processed_ids,
        "description": description,
        "source": source,
        "created_at": datetime.now().isoformat(),
        "total_chunks": len(chunk_ids),
        "completed_chunks": len(processed_ids)
    }
    _current_task = task_data
    
    task_file = _get_cache_dir() / f"task_{task_id}.json"
    with open(task_file, 'w', encoding='utf-8') as f:
        json.dump(task_data, f, ensure_ascii=False, indent=2)
    
    # åŒæ—¶ä¿å­˜ä¸ºå½“å‰ä»»åŠ¡
    current_task_file = _get_cache_dir() / "current_task.json"
    with open(current_task_file, 'w', encoding='utf-8') as f:
        json.dump(task_data, f, ensure_ascii=False, indent=2)


def _load_current_task() -> Optional[Dict[str, Any]]:
    """åŠ è½½å½“å‰ä»»åŠ¡çŠ¶æ€"""
    global _current_task
    if _current_task:
        return _current_task
    
    current_task_file = _get_cache_dir() / "current_task.json"
    if current_task_file.exists():
        with open(current_task_file, 'r', encoding='utf-8') as f:
            _current_task = json.load(f)
            return _current_task
    return None


async def get_processing_status() -> Dict[str, Any]:
    """è·å–å½“å‰æ•°æ®å¤„ç†ä»»åŠ¡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬å·²å¤„ç†å’Œæœªå¤„ç†çš„æ•°æ®å—"""
    task = _load_current_task()
    if not task:
        return {
            "success": False,
            "error": "æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ•°æ®å¤„ç†ä»»åŠ¡",
            "hint": "è¯·å…ˆä½¿ç”¨æ•°æ®æŸ¥è¯¢å·¥å…·è·å–æ•°æ®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ†å—"
        }
    
    chunk_ids = task.get("chunk_ids", [])
    processed_ids = task.get("processed_ids", [])
    unprocessed_ids = [cid for cid in chunk_ids if cid not in processed_ids]
    
    return {
        "success": True,
        "task_id": task.get("task_id"),
        "description": task.get("description", ""),
        "source": task.get("source", ""),
        "total_chunks": len(chunk_ids),
        "processed_count": len(processed_ids),
        "unprocessed_count": len(unprocessed_ids),
        "processed_ids": processed_ids,
        "unprocessed_ids": unprocessed_ids,
        "progress_percent": round(len(processed_ids) / len(chunk_ids) * 100, 1) if chunk_ids else 0,
        "message": f"å·²å¤„ç† {len(processed_ids)}/{len(chunk_ids)} ä¸ªæ•°æ®å—ï¼Œè¿˜å‰© {len(unprocessed_ids)} ä¸ªæœªå¤„ç†"
    }


async def mark_chunk_processed(chunk_id: str) -> Dict[str, Any]:
    """æ ‡è®°ä¸€ä¸ªæ•°æ®å—ä¸ºå·²å¤„ç†"""
    global _current_task
    task = _load_current_task()
    if not task:
        return {"success": False, "error": "æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ•°æ®å¤„ç†ä»»åŠ¡"}
    
    processed_ids = task.get("processed_ids", [])
    if chunk_id not in processed_ids:
        processed_ids.append(chunk_id)
        task["processed_ids"] = processed_ids
        task["completed_chunks"] = len(processed_ids)
        _current_task = task
        
        # æ›´æ–°æ–‡ä»¶
        current_task_file = _get_cache_dir() / "current_task.json"
        with open(current_task_file, 'w', encoding='utf-8') as f:
            json.dump(task, f, ensure_ascii=False, indent=2)
    
    chunk_ids = task.get("chunk_ids", [])
    remaining = len(chunk_ids) - len(processed_ids)
    
    return {
        "success": True,
        "message": f"å·²æ ‡è®° {chunk_id} ä¸ºå·²å¤„ç†",
        "processed_count": len(processed_ids),
        "remaining_count": remaining,
        "progress_percent": round(len(processed_ids) / len(chunk_ids) * 100, 1) if chunk_ids else 0
    }


async def get_next_unprocessed_chunk() -> Dict[str, Any]:
    """è·å–ä¸‹ä¸€ä¸ªæœªå¤„ç†çš„æ•°æ®å—å†…å®¹"""
    task = _load_current_task()
    if not task:
        return {"success": False, "error": "æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ•°æ®å¤„ç†ä»»åŠ¡"}
    
    chunk_ids = task.get("chunk_ids", [])
    processed_ids = task.get("processed_ids", [])
    unprocessed_ids = [cid for cid in chunk_ids if cid not in processed_ids]
    
    if not unprocessed_ids:
        return {
            "success": True,
            "all_processed": True,
            "message": "æ‰€æœ‰æ•°æ®å—å·²å¤„ç†å®Œæˆï¼è¯·è°ƒç”¨ dataproc_merge_summaries ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"
        }
    
    next_chunk_id = unprocessed_ids[0]
    chunk_result = await get_chunk(next_chunk_id)
    
    if chunk_result.get("success"):
        chunk_result["remaining_chunks"] = len(unprocessed_ids) - 1
        chunk_result["total_chunks"] = len(chunk_ids)
        chunk_result["processed_count"] = len(processed_ids)
        chunk_result["message"] = f"è¿™æ˜¯ç¬¬ {len(processed_ids) + 1}/{len(chunk_ids)} ä¸ªæ•°æ®å—ï¼Œå¤„ç†åè¿˜å‰© {len(unprocessed_ids) - 1} ä¸ª"
    
    return chunk_result


async def process_large_data(file_path: str = None, dir_path: str = None, 
                              pattern: str = "*.txt", task_description: str = "") -> Dict[str, Any]:
    """å¤„ç†å¤§æ•°æ®çš„å®Œæ•´æµç¨‹ï¼šåˆ†å—å¹¶è¿”å›å¤„ç†æŒ‡å—
    
    è¿™æ˜¯ä¸€ä¸ªé«˜çº§å·¥å…·ï¼Œç”¨äºå¤„ç†è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶çš„å¤§é‡æ•°æ®ã€‚
    å®ƒä¼šè‡ªåŠ¨åˆ†å—æ•°æ®ï¼Œå¹¶è¿”å›åç»­å¤„ç†çš„æ­¥éª¤æŒ‡å—ã€‚
    """
    try:
        if dir_path:
            result = await chunk_directory(dir_path, pattern)
        elif file_path:
            result = await chunk_file(file_path)
        else:
            return {"success": False, "error": "å¿…é¡»æä¾› file_path æˆ– dir_path"}
        
        if not result.get("success"):
            return result
        
        chunks = result.get("chunks", [])
        total_tokens = result.get("total_estimated_tokens", 0)
        
        instructions = f"""
æ•°æ®å·²åˆ†å—å®Œæˆï¼

ğŸ“Š åˆ†å—ç»Ÿè®¡:
- æ€»æ•°æ®å—: {len(chunks)} ä¸ª
- ä¼°è®¡æ€»Tokenæ•°: {total_tokens}
- æ¯å—çº¦: {total_tokens // len(chunks) if chunks else 0} tokens

ğŸ“‹ åç»­å¤„ç†æ­¥éª¤:
1. ä½¿ç”¨ dataproc_get_chunk å·¥å…·é€ä¸ªè·å–æ•°æ®å—å†…å®¹
2. å¯¹æ¯ä¸ªæ•°æ®å—è¿›è¡Œåˆ†æ/æ‘˜è¦ï¼Œä½¿ç”¨ dataproc_save_summary ä¿å­˜ç»“æœ
3. æ‰€æœ‰å—å¤„ç†å®Œæˆåï¼Œä½¿ç”¨ dataproc_get_all_summaries è·å–æ‰€æœ‰æ‘˜è¦
4. åŸºäºæ±‡æ€»çš„æ‘˜è¦ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

ğŸ”– æ•°æ®å—IDåˆ—è¡¨:
{chr(10).join([f"  - {c['chunk_id']} (æ¥æº: {c['source'][:30]}..., {c['estimated_tokens']} tokens)" for c in chunks[:10]])}
{'  ... è¿˜æœ‰ ' + str(len(chunks) - 10) + ' ä¸ªæ•°æ®å—' if len(chunks) > 10 else ''}

ğŸ’¡ ä»»åŠ¡æè¿°: {task_description or 'æœªæŒ‡å®š'}

è¯·é€ä¸ªå¤„ç†æ•°æ®å—ï¼Œå®Œæˆååˆå¹¶ç»“æœã€‚
"""
        
        return {
            "success": True,
            "total_chunks": len(chunks),
            "total_estimated_tokens": total_tokens,
            "chunk_ids": [c["chunk_id"] for c in chunks],
            "instructions": instructions
        }
    except Exception as e:
        logger.error(f"å¤„ç†å¤§æ•°æ®å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def register_tools() -> Dict[str, Any]:
    """æ³¨å†Œå·¥å…·å‡½æ•°"""
    return {
        "chunk_text": chunk_text,
        "chunk_file": chunk_file,
        "chunk_directory": chunk_directory,
        "get_chunk": get_chunk,
        "save_summary": save_summary,
        "get_all_summaries": get_all_summaries,
        "estimate_tokens": estimate_tokens,
        "clear_cache": clear_cache,
        "process_large_data": process_large_data,
        "get_status": get_processing_status,
        "get_next_chunk": get_next_unprocessed_chunk,
        "mark_processed": mark_chunk_processed
    }


def get_tool_definitions() -> list:
    """è·å–å·¥å…·å®šä¹‰"""
    return [
        {
            "type": "function",
            "function": {
                "name": "dataproc_process_large_data",
                "description": "å¤„ç†è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶çš„å¤§é‡æ•°æ®ã€‚è‡ªåŠ¨å°†æ•°æ®åˆ†å—å¹¶è¿”å›å¤„ç†æŒ‡å—ã€‚é€‚ç”¨äºåˆ†æå¤§æ–‡ä»¶ã€æ—¥æŠ¥æ±‡æ€»ç­‰åœºæ™¯ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "è¦å¤„ç†çš„å¤§æ–‡ä»¶è·¯å¾„"},
                        "dir_path": {"type": "string", "description": "è¦å¤„ç†çš„ç›®å½•è·¯å¾„ï¼ˆå¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰åŒ¹é…æ–‡ä»¶ï¼‰"},
                        "pattern": {"type": "string", "description": "æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œå¦‚ *.txt, *.mdï¼Œé»˜è®¤ *.txt"},
                        "task_description": {"type": "string", "description": "ä»»åŠ¡æè¿°ï¼Œè¯´æ˜éœ€è¦å¯¹æ•°æ®åšä»€ä¹ˆåˆ†æ"}
                    }
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_chunk_file",
                "description": "å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªæ•°æ®å—ï¼Œæ¯å—çº¦60000 tokensï¼Œç”¨äºåˆ†æ‰¹å¤„ç†",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "æ–‡ä»¶è·¯å¾„"}
                    },
                    "required": ["file_path"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_chunk_directory",
                "description": "å°†ç›®å½•ä¸­çš„å¤šä¸ªæ–‡ä»¶åˆ†å‰²æˆæ•°æ®å—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "dir_path": {"type": "string", "description": "ç›®å½•è·¯å¾„"},
                        "pattern": {"type": "string", "description": "æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œå¦‚ *.txt, *.md"}
                    },
                    "required": ["dir_path"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_get_chunk",
                "description": "è·å–æŒ‡å®šæ•°æ®å—çš„å®Œæ•´å†…å®¹ï¼Œç”¨äºå•ç‹¬åˆ†æå¤„ç†",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "chunk_id": {"type": "string", "description": "æ•°æ®å—ID"}
                    },
                    "required": ["chunk_id"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_save_summary",
                "description": "ä¿å­˜æ•°æ®å—çš„æ‘˜è¦/åˆ†æç»“æœ",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "chunk_id": {"type": "string", "description": "æ•°æ®å—ID"},
                        "summary": {"type": "string", "description": "æ‘˜è¦å†…å®¹"},
                        "key_points": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å…³é”®è¦ç‚¹åˆ—è¡¨"
                        }
                    },
                    "required": ["chunk_id", "summary"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_get_all_summaries",
                "description": "è·å–æ‰€æœ‰å·²ä¿å­˜çš„æ•°æ®å—æ‘˜è¦ï¼Œç”¨äºæœ€ç»ˆå½’çº¦åˆå¹¶ç”ŸæˆæŠ¥å‘Š",
                "parameters": {
                    "type": "object",
                    "properties": {}
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_estimate_tokens",
                "description": "ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "text": {"type": "string", "description": "éœ€è¦ä¼°ç®—çš„æ–‡æœ¬"}
                    },
                    "required": ["text"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_clear_cache",
                "description": "æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„æ•°æ®å—å’Œæ‘˜è¦",
                "parameters": {
                    "type": "object",
                    "properties": {}
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_get_status",
                "description": "è·å–å½“å‰æ•°æ®å¤„ç†ä»»åŠ¡çš„çŠ¶æ€ï¼Œæ˜¾ç¤ºå·²å¤„ç†å’Œæœªå¤„ç†çš„æ•°æ®å—æ•°é‡ã€‚ç”¨äºæŸ¥çœ‹å¤„ç†è¿›åº¦æˆ–ç»§ç»­æœªå®Œæˆçš„ä»»åŠ¡ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {}
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dataproc_get_next_chunk",
                "description": "è·å–ä¸‹ä¸€ä¸ªæœªå¤„ç†çš„æ•°æ®å—å†…å®¹ã€‚ç”¨äºç»§ç»­å¤„ç†å‰©ä½™æ•°æ®å—ï¼Œæ— éœ€è®°ä½chunk_idã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {}
                }
            }
        }
    ]


TOOLS = register_tools()
